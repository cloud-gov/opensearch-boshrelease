
# # gunzip
# ruby {
#     code => "
#     require 'zlib'
#     require 'stringio'
#     begin
#         gz = Zlib::GzipReader.new(StringIO.new(event.get('message').to_s.b))
#         decompressed = gz.read
#         gz.close
#         event.set('message', decompressed)
#     rescue => e
#         event.tag('gzip_failed')
#         event.set('gzip_error', e.message)
#     end
#     "
# }

# base64 decode
# ruby {
#     code => "
#     require 'base64'
#     begin
#         decoded = Base64.decode64(event.get('message'))
#         event.set('message',decoded)
#     rescue => e
#         event.tag('base64_decode_failed')
#         event.set('decoded_error', e.message)
#     end
#     "
# }

ruby {
  code => 'event.set("test", "ruby_works")'
}

  ruby {
    code => '
      require "zlib"
      require "json"

      begin
        # Get the gzipped binary data
        compressed_data = event.get("message")

        # Decompress
        decompressed = Zlib::Inflate.inflate(compressed_data)

        # Parse concatenated JSON
        objects = []
        decoder = JSON::JSONDecoder.new
        idx = 0

        while idx < decompressed.length
          content_slice = decompressed[idx..-1].lstrip
          break if content_slice.empty?

          begin
            obj, end_idx = decoder.raw_decode(content_slice)
            objects << obj
            idx += (decompressed.length - content_slice.length + end_idx)
          rescue JSON::ParserError => e
            event.set("json_parse_error", e.message)
            event.tag("_jsonparsefailure")
            break
          end
        end

        # Create events for each JSON object
        objects.each_with_index do |json_obj, index|
          new_event = LogStash::Event.new(json_obj)
          new_event.set("[@metadata][original_s3_key]", event.get("[@metadata][s3][key]"))
          new_event.set("[@metadata][json_object_number]", index + 1)

          yield new_event
        end

        event.cancel

      rescue Zlib::Error => e
        event.set("gzip_error", e.message)
        event.tag("_gziperror")
      rescue => e
        event.set("processing_error", e.message)
        event.tag("_processingerror")
      end
    '
  }


json
{
    source => "message"
    tag_on_failure => ["_jsonparsefailure"]
}

date
{
match => ["Time", "ISO8601"]
target => "@timestamp"
}
mutate
{
add_field => {"@type" => "metrics"}
rename => {"[Tags][Organization GUID]"=>"[@cf][org_id]"}
rename => {"[Tags][Organization name]"=>"[@cf][org]"}
rename => {"[Tags][Space GUID]"=>"[@cf][space_id]"}
rename => {"[Tags][Space name]"=>"[@cf][space]"}
rename => {"[Tags][Service plan name]"=>"[@cf][service_plan]"}
rename => {"[Tags][Plan GUID]"=>"[@cf][plan_id]"}
rename => {"[Tags][Service GUID]"=>"[@cf][service_instance_id]"}
rename => {"[Tags][service]"=>"[@cf][service]"}
rename => {"[Tags][Service offering name]"=>"[@cf][service_offering]"}
rename => {"[Tags][Instance GUID]"=>"[@cf][instance_id]"}
rename => {"[Tags][Instance name]"=>"[@cf][instance]"}

rename => {"[Tags][Created at]" => "[created at]"}
rename => {"[Tags][broker]"=>"[broker]"}
rename => {"[Tags][environment]"=>"environment"}
rename => {"[Tags][DomainName]"=>"[metric][domain_name]"}
rename => {"[InstanceName]"=>"[metric][instance_id]"}
rename => {"[MetricName]"=>"[metric][name]"}
rename => {"[Average]"=>"[metric][average]"}
rename => {"[Unit]"=>"[metric][unit]"}
remove_field => ["[Tags][client]"]
}