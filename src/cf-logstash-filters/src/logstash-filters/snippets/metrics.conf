
# # gunzip
# ruby {
#     code => "
#     require 'zlib'
#     require 'stringio'
#     begin
#         gz = Zlib::GzipReader.new(StringIO.new(event.get('message').to_s.b))
#         decompressed = gz.read
#         gz.close
#         event.set('message', decompressed)
#     rescue => e
#         event.tag('gzip_failed')
#         event.set('gzip_error', e.message)
#     end
#     "
# }

# base64 decode
# ruby {
#     code => "
#     require 'base64'
#     begin
#         decoded = Base64.decode64(event.get('message'))
#         event.set('message',decoded)
#     rescue => e
#         event.tag('base64_decode_failed')
#         event.set('decoded_error', e.message)
#     end
#     "
# }
ruby {
    code => '
      require "zlib"
      require "json"

      begin
        # Get the raw message content (gzipped data)
        compressed_data = event.get("message")

        # Decompress the gzipped content
        decompressed = Zlib::Inflate.inflate(compressed_data)

        # Split into lines and parse each JSON object
        lines = decompressed.split("\n")

        lines.each_with_index do |line, index|
          next if line.strip.empty?

          begin
            parsed = JSON.parse(line)

            # Create a new event for each metric
            new_event = LogStash::Event.new(parsed)
            new_event.set("[@metadata][original_s3_key]", event.get("[@metadata][s3][key]"))
            new_event.set("[@metadata][line_number]", index)

            # Yield the new event to the pipeline
            yield new_event

          rescue JSON::ParserError => e
            # Log parsing errors but continue processing
            event.set("json_parse_error", e.message)
            event.set("failed_line", line)
            event.tag("_jsonparsefailure")
          end
        end

        # Cancel the original event since we created new ones
        event.cancel

      rescue => e
        event.set("processing_error", e.message)
        event.tag("_processingerror")
      end
    '
  }

json
{
    source => "message"
    tag_on_failure => ["_jsonparsefailure"]
}

date
{
match => ["Time", "ISO8601"]
target => "@timestamp"
}
mutate
{
add_field => {"@type" => "metrics"}
rename => {"[Tags][Organization GUID]"=>"[@cf][org_id]"}
rename => {"[Tags][Organization name]"=>"[@cf][org]"}
rename => {"[Tags][Space GUID]"=>"[@cf][space_id]"}
rename => {"[Tags][Space name]"=>"[@cf][space]"}
rename => {"[Tags][Service plan name]"=>"[@cf][service_plan]"}
rename => {"[Tags][Plan GUID]"=>"[@cf][plan_id]"}
rename => {"[Tags][Service GUID]"=>"[@cf][service_instance_id]"}
rename => {"[Tags][service]"=>"[@cf][service]"}
rename => {"[Tags][Service offering name]"=>"[@cf][service_offering]"}
rename => {"[Tags][Instance GUID]"=>"[@cf][instance_id]"}
rename => {"[Tags][Instance name]"=>"[@cf][instance]"}

rename => {"[Tags][Created at]" => "[created at]"}
rename => {"[Tags][broker]"=>"[broker]"}
rename => {"[Tags][environment]"=>"environment"}
rename => {"[Tags][DomainName]"=>"[metric][domain_name]"}
rename => {"[InstanceName]"=>"[metric][instance_id]"}
rename => {"[MetricName]"=>"[metric][name]"}
rename => {"[Average]"=>"[metric][average]"}
rename => {"[Unit]"=>"[metric][unit]"}
remove_field => ["[Tags][client]"]
}